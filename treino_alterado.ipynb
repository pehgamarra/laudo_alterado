{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe20bf4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09bb79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a999aa",
   "metadata": {},
   "source": [
    "# DataFrame Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6c385",
   "metadata": {},
   "source": [
    "Carregando Xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('check_laudo.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eff5d77",
   "metadata": {},
   "source": [
    "Separando texto e label para treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef82d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['laudo'].tolist()\n",
    "labels = df['laudo alterado'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef499f",
   "metadata": {},
   "source": [
    "# Tokenização\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bca80b",
   "metadata": {},
   "source": [
    "Chamando o tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f425d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a54b6a",
   "metadata": {},
   "source": [
    "# Configurando o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e23c3578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaudoDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16787b97",
   "metadata": {},
   "source": [
    "# Split dos dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8866415",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "train_dataset = LaudoDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = LaudoDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ae910",
   "metadata": {},
   "source": [
    "# Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a18df4d",
   "metadata": {},
   "source": [
    "# Otimizador\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b1705",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e60a88",
   "metadata": {},
   "source": [
    "# Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59478d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 83/83 [1:31:25<00:00, 66.09s/it, Batch Loss=0.4780]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed - Loss: 0.5610 - Time: 5485.75 sec\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 83/83 [49:34<00:00, 35.84s/it, Batch Loss=0.8633]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed - Loss: 0.3723 - Time: 2974.91 sec\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 83/83 [43:47<00:00, 31.66s/it, Batch Loss=0.0115]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed - Loss: 0.1768 - Time: 2627.81 sec\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 83/83 [33:35<00:00, 24.28s/it, Batch Loss=1.9669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed - Loss: 0.1224 - Time: 2015.51 sec\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 83/83 [32:28<00:00, 23.47s/it, Batch Loss=0.0038]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed - Loss: 0.0582 - Time: 1948.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "    total_loss = 0\n",
    "\n",
    "    # Barra de progresso\n",
    "    progress_bar = tqdm(train_loader, desc='Training', leave=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Atualizar barra com o loss atual\n",
    "        progress_bar.set_postfix({'Batch Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1} completed - Loss: {avg_loss:.4f} - Time: {epoch_time:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5a7c46",
   "metadata": {},
   "source": [
    "# Avaliação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879663a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.8787878787878788\n",
      "[[ 38  17]\n",
      " [  3 107]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.69      0.79        55\n",
      "           1       0.86      0.97      0.91       110\n",
      "\n",
      "    accuracy                           0.88       165\n",
      "   macro avg       0.89      0.83      0.85       165\n",
      "weighted avg       0.88      0.88      0.87       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "print(\"Acurácia:\", accuracy_score(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1c3fb3",
   "metadata": {},
   "source": [
    "# Salvar modelo e tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8dffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('modelo_laudos\\\\tokenizer_config.json',\n",
       " 'modelo_laudos\\\\special_tokens_map.json',\n",
       " 'modelo_laudos\\\\vocab.txt',\n",
       " 'modelo_laudos\\\\added_tokens.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('modelo_laudos')\n",
    "tokenizer.save_pretrained('modelo_laudos')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
